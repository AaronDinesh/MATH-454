\section{Introduction}
In this assignment, a serial implementation of the Conjugate Gradient Method (CGM) was provided, which required
parallelization. In order to parallelize this implementation OpenMPI and the MPI parallelization paradigm was used.

\section{Serial Analysis}
The serial implementation of the CGM was evaluated using the provided \lstinline[language=C]|lap2D_5pt_n1000.mtx| matrix. This matrix
was chosen as its large size allowed for obvious parallelization opportunities to be revealed. The \lstinline[language=C]|perf|
profiling tool was used to analyse the serial code's execution time. From the profiling results, the \lstinline[language=C]|mat_vec|
function was identified as the primary computational bottleneck, accounting for approximately 86.27\% of the total execution
time. Consequently, the serial fraction of the program was estimated to be 13.73\%. The testing for the serial and
parallel code were performed on the Jed Cluster with and Intel Xeon Platinum 8360Y CPU, running at  2.40 GHz.


These values along with Amdahl's and Gustafson's laws for strong and weak scaling were used to calculate a theoretical
upper bound on the possible speed-ups that could be achieved using parallelization. The speed-ups were calculated using
the following equations~\cite{AmdahlGustafson2023}: 
\begin{align*}
   \text{Amdahl's Law (Strong Scaling)} = \frac{1}{(1-\alpha) + \frac{\alpha}{p}} 
\end{align*}
Here $\alpha$ is the fraction of the code that can be parallelized and $p$ is the number of processors. Gustafson's
equations are as follows:
\begin{align*}
    \text{Gustafson's Law (Weak Scaling)} = (1-\alpha) + \alpha p 
\end{align*}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{plots/scaling_laws.pdf}
    \caption{Strong and Weak Scaling Analysis using Amdahl's and Gustafson's Laws}
    \label{fig:AmdahlPlot}
\end{figure}
\FloatBarrier

\section{Parallel Modifications}
In order to parallelize the code with the least amount of communications, it was decided that the best course of action
would be to read in the matrix on processor 0 and then block distribute the row index, column index and value array to
each processor. Doing this when reading the matrix would implicitly parallelize the \lstinline[language=C]|mat_vec|
function and would reduce the communication needed to requiring just one \lstinline[language=C]|MPI_Allgather| after
each call to \lstinline[language=C]|mat_vec|, rather than a \lstinline[language=C]|MPI_Scatterv| and a
\lstinline[language=C]|MPI_Allgather| before and after the \lstinline[language=C]|mat_vec| calls. MPI message
packing and unpacking were also used to speed up this initial data transfer. This was enough to show some initial
improvements over the serial execution of the code, however, there was a lot more that could be done in the
\lstinline[language=C]|CGSolverSparse::solve()| function to further improve the parallelization of the code. Further
optimization was achieved through the use non-blocking transfers such as \lstinline[language=C]|MPI_Iallgatherv| as
well as combined communications such as \lstinline[language=C]|MPI_Reduce_scatter|. Using these combined
communications allows the MPI compiler to perform further optimizations when compiling the code. I also made use of
cblas operations such as \lstinline[language=C]|cblas_dcopy|, \lstinline[language=C]|cblas_daxpy| and
\lstinline[language=C]|cblas_ddot| to make use of the optimized implementations in the cblas library.

\section{Parallel Performance}
It was expected that the modifications introduced to the codebase would result in an improvement over sequential
runtimes. However, experimental results did not support this assumption. During testing, the parallel implementation
demonstrated poor scaling behaviour on the Jed Cluster. To illustrate this, strong and weak scaling experiments were
conducted. For Strong Scaling, the matrix \lstinline[language=C]|lap2D_5pt_n1000.mtx| and \lstinline[language=C]|lap2D_5pt_n200.mtx| were used, with the processor
counts ranging from 1 to 64 in powers of 2. The upper limit of 64 was selected based on the fact that no meaningful
scaling was observed past this point. In most configurations, the parallel solver performed comparably to the serial
implementation however in some cases, performance was slightly worse. The speed-up in the strong scaling case was
calculated using the following formula:
\begin{equation*}
    S(p) = \frac{T_1}{T_p}
\end{equation*}

where the subscript denotes the number of processors used. For the weak scaling experiments, the run time of a single
processor was tested with the \lstinline[language=C]|lap2D_5pt_n100.mtx| matrix. Then the processor count was raised
in powers of 2 and they were tested against the other matrices provided. From these experiments the runtime and the
iteration count to convergence were calculated. The speed-up in the weak scaling case is then calculated as follows:
\begin{equation*}
    S(p) = \frac{T_1 / I_1}{T_p / I_p}    
\end{equation*}

Where $I_{p}$ is the iteration count to convergence for $p$ processors. Since weak scaling requires the work done at
each test to be constant, the runtime was divided by the iteration count. The results form these experiments are shown
in the graphs below.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{plots/strong_scaling.pdf}
    \caption{Strong Scaling Analysis}
    \label{fig:StrongScaling}
\end{figure}
\FloatBarrier

As can be seen from the graph above, minor performance gains can be achieved by switching from a serial code to a few
parallel processes. However, past 8 processors no further improvements are observed. A similar story can also be seen
when performing a weak scaling analysis, as can be seen in the graph below. \\

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{plots/weak_scaling.pdf}
    \caption{Weak Scaling Analysis}
    \label{fig:WeakScaling}
\end{figure}
\FloatBarrier

The raw results of the strong and weak scaling analysis can be seen in the tables below. \\

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \diagbox{Matrix Size}{\# Procs} & 1 & 2 & 4 & 8 & 16 & 32 & 64 \\ \hline
        n200 & 1.00 & 1.13 & 1.17 & 0.94 & 0.49 & 0.51 & 0.39 \\ \hline
        n1000 & 1.00 & 1.27 & 1.49 & 1.26 & 1.00 & 0.85 & 0.54 \\ \hline
    \end{tabular}
    \caption{Strong scaling speedup for different matrix sizes}
\end{table}

        
% weak scaling speed up data [1. 0.25147243 0.10058002 0.0122578  0.00355457 0.00252197 0.00238294]

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \# Procs & 1 & 2 & 4 & 8 & 16 & 32 & 64 \\ \hline
        speedup & 1. & 0.25147243 & 0.10058002 & 0.0122578 & 0.00355457 & 0.00252197 & 0.00238294 \\ \hline
    \end{tabular}
    \caption{Weak scaling speedup}
\end{table}

It is suspected that the small initial performance gains observed in the strong and weak scaling analysis are primarily
due to the distribution of the sparse matrix across all participating processors, allowing for some performance gains in
the \lstinline[language=C]|mat_vec| function. However, this advantage is quickly overshadowed by the substantial
cost incurred by the collective communication required to calculate the full matrix vector product. As the
dimensionality of the system increases the volume of data involved in these collectives grows correspondingly, leading
to significant communication latency that dominates in the overall runtime, especially when considering strong scaling.
\\
To address this issue, several mitigation strategies were explored. The first made use of non-blocking communications
such as \lstinline[language=C]|MPI_Iallreduce|, to allow for independent work to be completed while the communication continued in the
background. While this yielded some improvements, the gains were minor due to the relatively tight coupling between
global reductions and subsequent computations. The second made significant use of optimized BLAS functions. The
reasoning behind this was that the BLAS functions were optimized for operations on large vectors and matrices. However,
as was the case before, the performance gains were minor, yielding further credibility to the idea that that main
bottleneck is the communication overhead. \\
A more effective solution, however, would be to make use of modified CG algorithms that are better suited for parallel
execution. For example the Global Reduction pipelining CG algorithm proposed by Cool and Cornelis et al.
\cite{cools2019improvingstrongscalingconjugate} reformulates the classic CG iteration to allow for multiple operations
to be overlapped with global communication. They introduce an auxiliary Krylov basis and structure the algorithm to
delay the use of reduction results by $\ell$ iterations, allowing them to overlap $\ell$ operations with a single
\lstinline[language=C]|MPI_Iallreduce| call. Through the use of this algorithm, the team demonstrates strong scaling
results up to 1024 nodes with 16 MPI ranks per node on the US Department of Energy's NERSC ``Cori'' machine.